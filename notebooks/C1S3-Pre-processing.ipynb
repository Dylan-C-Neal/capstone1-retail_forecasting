{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing - Retail Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to correct directory\n",
    "os.chdir('../data/interim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off setting with copy warning\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>20094.19</td>\n",
       "      <td>0</td>\n",
       "      <td>71.89</td>\n",
       "      <td>2.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.671989</td>\n",
       "      <td>7.838</td>\n",
       "      <td>2</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>45829.02</td>\n",
       "      <td>0</td>\n",
       "      <td>71.89</td>\n",
       "      <td>2.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.671989</td>\n",
       "      <td>7.838</td>\n",
       "      <td>2</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>9775.17</td>\n",
       "      <td>0</td>\n",
       "      <td>71.89</td>\n",
       "      <td>2.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.671989</td>\n",
       "      <td>7.838</td>\n",
       "      <td>2</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>34912.45</td>\n",
       "      <td>0</td>\n",
       "      <td>71.89</td>\n",
       "      <td>2.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.671989</td>\n",
       "      <td>7.838</td>\n",
       "      <td>2</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>23381.38</td>\n",
       "      <td>0</td>\n",
       "      <td>71.89</td>\n",
       "      <td>2.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.671989</td>\n",
       "      <td>7.838</td>\n",
       "      <td>2</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept        Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  \\\n",
       "0      1     1  2010-01-10      20094.19          0        71.89       2.603   \n",
       "1      1     2  2010-01-10      45829.02          0        71.89       2.603   \n",
       "2      1     3  2010-01-10       9775.17          0        71.89       2.603   \n",
       "3      1     4  2010-01-10      34912.45          0        71.89       2.603   \n",
       "4      1     5  2010-01-10      23381.38          0        71.89       2.603   \n",
       "\n",
       "   MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN  211.671989   \n",
       "1        NaN        NaN        NaN        NaN        NaN  211.671989   \n",
       "2        NaN        NaN        NaN        NaN        NaN  211.671989   \n",
       "3        NaN        NaN        NaN        NaN        NaN  211.671989   \n",
       "4        NaN        NaN        NaN        NaN        NaN  211.671989   \n",
       "\n",
       "   Unemployment  Type    Size  Month  Week  \n",
       "0         7.838     2  151315      1     1  \n",
       "1         7.838     2  151315      1     1  \n",
       "2         7.838     2  151315      1     1  \n",
       "3         7.838     2  151315      1     1  \n",
       "4         7.838     2  151315      1     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data and inspect head\n",
    "df = pd.read_csv('data post-eda.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach is going to be dropping the markdown columns in order to have access to the entire data set for training.\n",
    "\n",
    "I plan to create 3 separate models, one for each of the different store types. My hope is to limit the scope of each model to improve individual peformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping markdown columns\n",
    "df.drop(columns=['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dummy Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that should have dummy features encoded:\n",
    "- Store\n",
    "- Dept\n",
    "- Type\n",
    "- Month\n",
    "- Week\n",
    "\n",
    "Store dummy features will be generated after splitting the data by store type. This will reduce the number of unnecessary features in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy features\n",
    "deptDummies = pd.get_dummies(df['Dept'], prefix='Dept', drop_first=True)\n",
    "monthDummies = pd.get_dummies(df['Month'], prefix='Month', drop_first=True)\n",
    "weekDummies = pd.get_dummies(df['Week'], prefix='Week', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join dummy features onto dataframe\n",
    "df = df.join([deptDummies, monthDummies, weekDummies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original features\n",
    "df.drop(columns=['Dept', 'Month', 'Week'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data by 3 store types\n",
    "df0 = df.loc[df.Type == 0]\n",
    "df1 = df.loc[df.Type == 1]\n",
    "df2 = df.loc[df.Type == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Type and Date features\n",
    "df0 = df0.drop(columns=['Type'])\n",
    "df1 = df1.drop(columns=['Type'])\n",
    "df2 = df2.drop(columns=['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Store Dummy Features\n",
    "storeDummies0 = pd.get_dummies(df0['Store'], prefix='Store', drop_first=True)\n",
    "storeDummies1 = pd.get_dummies(df1['Store'], prefix='Store', drop_first=True)\n",
    "storeDummies2 = pd.get_dummies(df2['Store'], prefix='Store', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Store dummes to dataframes\n",
    "df0 = df0.join(storeDummies0)\n",
    "df1 = df1.join(storeDummies1)\n",
    "df2 = df2.join(storeDummies2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Store columns from dataframes\n",
    "df0 = df0.drop(columns=['Store'])\n",
    "df1 = df1.drop(columns=['Store'])\n",
    "df2 = df2.drop(columns=['Store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42597, 155)\n",
      "(163495, 166)\n",
      "(215478, 171)\n"
     ]
    }
   ],
   "source": [
    "# Print dataframe shapes\n",
    "print(df0.shape)\n",
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to perform a train test split prior to scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks\n",
    "trainmask0 = df0.Date < '2012-01-01'\n",
    "testmask0 = df0.Date >= '2012-01-01'\n",
    "trainmask1 = df1.Date < '2012-01-01'\n",
    "testmask1 = df1.Date >= '2012-01-01'\n",
    "trainmask2 = df2.Date < '2012-01-01'\n",
    "testmask2 = df2.Date >= '2012-01-01'\n",
    "\n",
    "# Split into sets by masks\n",
    "dftrain0 = df0.loc[trainmask0]\n",
    "dftest0 = df0.loc[testmask0]\n",
    "dftrain1 = df1.loc[trainmask1]\n",
    "dftest1 = df1.loc[testmask1]\n",
    "dftrain2 = df2.loc[trainmask2]\n",
    "dftest2 = df2.loc[testmask2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaler transformation for continuous feature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler and fit_transform X_train\n",
    "scaler0 = MinMaxScaler()\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "\n",
    "# List of features to transform\n",
    "flist = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size']\n",
    "\n",
    "# Fit and transform training data\n",
    "dftrain0[flist] = scaler0.fit_transform(dftrain0[flist])\n",
    "dftrain1[flist] = scaler1.fit_transform(dftrain1[flist])\n",
    "dftrain2[flist] = scaler2.fit_transform(dftrain2[flist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Size</th>\n",
       "      <th>Dept_2</th>\n",
       "      <th>Dept_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Week_48</th>\n",
       "      <th>Week_49</th>\n",
       "      <th>Week_50</th>\n",
       "      <th>Week_51</th>\n",
       "      <th>Week_52</th>\n",
       "      <th>Store_37</th>\n",
       "      <th>Store_38</th>\n",
       "      <th>Store_42</th>\n",
       "      <th>Store_43</th>\n",
       "      <th>Store_44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>9843.45</td>\n",
       "      <td>0</td>\n",
       "      <td>0.648274</td>\n",
       "      <td>0.052725</td>\n",
       "      <td>0.915722</td>\n",
       "      <td>0.253188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>13898.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.648274</td>\n",
       "      <td>0.052725</td>\n",
       "      <td>0.915722</td>\n",
       "      <td>0.253188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>755.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0.648274</td>\n",
       "      <td>0.052725</td>\n",
       "      <td>0.915722</td>\n",
       "      <td>0.253188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>13610.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0.648274</td>\n",
       "      <td>0.052725</td>\n",
       "      <td>0.915722</td>\n",
       "      <td>0.253188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>444.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0.648274</td>\n",
       "      <td>0.052725</td>\n",
       "      <td>0.915722</td>\n",
       "      <td>0.253188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price       CPI  \\\n",
       "1993  2010-01-10       9843.45          0     0.648274    0.052725  0.915722   \n",
       "1994  2010-01-10      13898.10          0     0.648274    0.052725  0.915722   \n",
       "1995  2010-01-10        755.67          0     0.648274    0.052725  0.915722   \n",
       "1996  2010-01-10      13610.70          0     0.648274    0.052725  0.915722   \n",
       "1997  2010-01-10        444.76          0     0.648274    0.052725  0.915722   \n",
       "\n",
       "      Unemployment  Size  Dept_2  Dept_3  ...  Week_48  Week_49  Week_50  \\\n",
       "1993      0.253188   1.0       0       0  ...        0        0        0   \n",
       "1994      0.253188   1.0       1       0  ...        0        0        0   \n",
       "1995      0.253188   1.0       0       1  ...        0        0        0   \n",
       "1996      0.253188   1.0       0       0  ...        0        0        0   \n",
       "1997      0.253188   1.0       0       0  ...        0        0        0   \n",
       "\n",
       "      Week_51  Week_52  Store_37  Store_38  Store_42  Store_43  Store_44  \n",
       "1993        0        0         0         0         0         0         0  \n",
       "1994        0        0         0         0         0         0         0  \n",
       "1995        0        0         0         0         0         0         0  \n",
       "1996        0        0         0         0         0         0         0  \n",
       "1997        0        0         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that tranformation was successful\n",
    "dftrain0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply scaler transformation to test data\n",
    "dftest0[flist] = scaler0.transform(dftest0[flist])\n",
    "dftest1[flist] = scaler1.transform(dftest1[flist])\n",
    "dftest2[flist] = scaler2.transform(dftest2[flist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X & y\n",
    "\n",
    "# Store type 0\n",
    "X_train0 = dftrain0.drop(columns=['Date', 'Weekly_Sales'])\n",
    "y_train0 = dftrain0[['Weekly_Sales']]\n",
    "X_test0 = dftest0.drop(columns=['Date', 'Weekly_Sales'])\n",
    "y_test0 = dftest0[['Weekly_Sales']]\n",
    "\n",
    "# Store type 1\n",
    "X_train1 = dftrain1.drop(columns=['Date', 'Weekly_Sales'])\n",
    "y_train1 = dftrain1[['Weekly_Sales']]\n",
    "X_test1 = dftest1.drop(columns=['Date', 'Weekly_Sales'])\n",
    "y_test1 = dftest1[['Weekly_Sales']]\n",
    "\n",
    "# Store type 2\n",
    "X_train2 = dftrain2.drop(columns=['Date', 'Weekly_Sales'])\n",
    "y_train2 = dftrain2[['Weekly_Sales']]\n",
    "X_test2 = dftest2.drop(columns=['Date', 'Weekly_Sales'])\n",
    "y_test2 = dftest2[['Weekly_Sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Cross-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function for splitting the training data into segments in order to perform cross-validation on models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvsplit(df):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    # Create masks for 5-fold CV\n",
    "    trainm1 = df.Date < '2011-01-01'\n",
    "    testm1 = (df.Date >= '2011-01-01') & (df.Date < '2011-03-01')\n",
    "    trainm2 = df.Date < '2011-03-01'\n",
    "    testm2 = (df.Date >= '2011-03-01') & (df.Date < '2011-05-01')\n",
    "    trainm3 = df.Date < '2011-05-01'\n",
    "    testm3 = (df.Date >= '2011-05-01') & (df.Date < '2011-07-01')\n",
    "    trainm4 = df.Date < '2011-07-01'\n",
    "    testm4 = (df.Date >= '2011-07-01') & (df.Date < '2011-09-01')\n",
    "    trainm5 = df.Date < '2011-09-01'\n",
    "    testm5 = (df.Date >= '2011-09-01') & (df.Date < '2012-01-01')\n",
    "    \n",
    "    # Split into sets by masks\n",
    "    dftrain1 = df.loc[trainm1]\n",
    "    dftest1 = df.loc[testm1]    \n",
    "    dftrain2 = df.loc[trainm2]\n",
    "    dftest2 = df.loc[testm2]   \n",
    "    dftrain3 = df.loc[trainm3]\n",
    "    dftest3 = df.loc[testm3]   \n",
    "    dftrain4 = df.loc[trainm4]\n",
    "    dftest4 = df.loc[testm4]   \n",
    "    dftrain5 = df.loc[trainm5]\n",
    "    dftest5 = df.loc[testm5]\n",
    "    \n",
    "    # Test train split and append to lists\n",
    "    X_train.append(dftrain1.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_train.append(dftrain1[['Weekly_Sales']])\n",
    "    X_test.append(dftest1.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_test.append(dftest1[['Weekly_Sales']])\n",
    "    \n",
    "    X_train.append(dftrain2.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_train.append(dftrain2[['Weekly_Sales']])\n",
    "    X_test.append(dftest2.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_test.append(dftest2[['Weekly_Sales']])\n",
    "    \n",
    "    X_train.append(dftrain3.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_train.append(dftrain3[['Weekly_Sales']])\n",
    "    X_test.append(dftest3.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_test.append(dftest3[['Weekly_Sales']])\n",
    "    \n",
    "    X_train.append(dftrain4.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_train.append(dftrain4[['Weekly_Sales']])\n",
    "    X_test.append(dftest4.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_test.append(dftest4[['Weekly_Sales']])\n",
    "    \n",
    "    X_train.append(dftrain5.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_train.append(dftrain5[['Weekly_Sales']])\n",
    "    X_test.append(dftest5.drop(columns=['Date', 'Weekly_Sales']))\n",
    "    y_test.append(dftest5[['Weekly_Sales']])\n",
    "    \n",
    "    # Return CV test train lists\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation sets\n",
    "cvX_train0, cvy_train0, cvX_test0, cvy_test0 = cvsplit(dftrain0)\n",
    "cvX_train1, cvy_train1, cvX_test1, cvy_test1 = cvsplit(dftrain1)\n",
    "cvX_train2, cvy_train2, cvX_test2, cvy_test2 = cvsplit(dftrain2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Pre-Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0.to_csv('X_train0.csv', index=False)\n",
    "y_train0.to_csv('y_train0.csv', index=False)\n",
    "X_test0.to_csv('X_test0.csv', index=False)\n",
    "y_test0.to_csv('y_test0.csv', index=False)\n",
    "pickle.dump(cvX_train0, open(\"cvX_train0\", \"wb\"))\n",
    "pickle.dump(cvy_train0, open(\"cvy_train0\", \"wb\"))\n",
    "pickle.dump(cvX_test0, open(\"cvX_test0\", \"wb\"))\n",
    "pickle.dump(cvy_test0, open (\"cvy_test0\", \"wb\"))\n",
    "\n",
    "X_train1.to_csv('X_train1.csv', index=False)\n",
    "y_train1.to_csv('y_train1.csv', index=False)\n",
    "X_test1.to_csv('X_test1.csv', index=False)\n",
    "y_test1.to_csv('y_test1.csv', index=False)\n",
    "pickle.dump(cvX_train1, open(\"cvX_train1\", \"wb\"))\n",
    "pickle.dump(cvy_train1, open(\"cvy_train1\", \"wb\"))\n",
    "pickle.dump(cvX_test1, open(\"cvX_test1\", \"wb\"))\n",
    "pickle.dump(cvy_test1, open(\"cvy_test1\", \"wb\"))\n",
    "\n",
    "X_train2.to_csv('X_train2.csv', index=False)\n",
    "y_train2.to_csv('y_train2.csv', index=False)\n",
    "X_test2.to_csv('X_test2.csv', index=False)\n",
    "y_test2.to_csv('y_test2.csv', index=False)\n",
    "pickle.dump(cvX_train2, open(\"cvX_train2\", \"wb\"))\n",
    "pickle.dump(cvy_train2, open(\"cvy_train2\", \"wb\"))\n",
    "pickle.dump(cvX_test2, open(\"cvX_test2\", \"wb\"))\n",
    "pickle.dump(cvy_test2, open(\"cvy_test2\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
